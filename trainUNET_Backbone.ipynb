{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf.keras'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import segmentation_models as sm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sm.set_framework('tf.keras')\n",
    "\n",
    "sm.framework()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the backbone for UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE_3 = 'resnet152'\n",
    "preprocessing_input = sm.get_preprocessing(BACKBONE_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=24                   # setting seed \n",
    "batch_size= 4             # batch_size for training\n",
    "n_classes=9               # number of class/ labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalization\n",
    "scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the input images\n",
    "def data_preprocessing(img, mask, num_class):\n",
    "    \n",
    "    #Scale images  \n",
    "    #img = img[0:512,0:512]\n",
    "    img = scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)   # fit and transform image using MinMaxScaler\n",
    "    \n",
    "    # preprocessing the input if a backbone is used else comment the line below if you want to use just UNET\n",
    "    img = preprocessing_input(img)\n",
    "    \n",
    "    #mask = mask[0:512,0:512]\n",
    "    # label encoding for the mask image\n",
    "    labelencoder = LabelEncoder()                                                   # initializing Labelencoder\n",
    "    number_of_images, height, width, channles= mask.shape                           # shape of the mask image\n",
    "    mask_reshape = mask.reshape(-1,1)                                               # reshaping the mask image numpy array\n",
    "    encoded_mask = labelencoder.fit_transform(mask_reshape.ravel())                 # fit and transform mask image using label encoder\n",
    "    original_encoded_mask = encoded_mask.reshape(number_of_images, height, width )  # reshaping the image numpy array\n",
    "    mask = np.expand_dims(original_encoded_mask, axis = 3)                          # expanding dimension (requirement by the model)\n",
    "                                                                    \n",
    "    #Convert mask to one-hot encoding\n",
    "    mask = to_categorical(mask, num_class)\n",
    "                                        # into to categorical pixel values\n",
    "    return (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the data loader\n",
    "def TFDataLoader(train_img_path, train_mask_path, num_class):\n",
    "    \n",
    "    # augmention parameters for images\n",
    "    img_data_gen_args = dict(\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    fill_mode='nearest'\n",
    "                      )\n",
    "\n",
    "    # initializing ImageDataGenerator for both images and masks\n",
    "    image_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    \n",
    "    # images will be loaded directly from the local drive (less load on the memory)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_img_path,\n",
    "        target_size=(256, 256),     # for PSP net ,shape should be divisible by 48\n",
    "        class_mode = None,\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_mask_path,\n",
    "        target_size=(256, 256),\n",
    "        class_mode = None,\n",
    "        color_mode = 'grayscale',\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "        \n",
    "    # zip both images and mask \n",
    "    data_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    for (img, mask) in data_generator:\n",
    "        img, mask = data_preprocessing(img, mask, num_class)\n",
    "        #mask = mask[:,:,:,1:] # to remove background!\n",
    "        yield (img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for both train, val datasets and test dataset\n",
    "\n",
    "# for scale.rapid\n",
    "\n",
    "train_img_path = \"../../data/unet_img/Data_TF_Scalerapid_Split/train_image/\"                    \n",
    "train_mask_path = \"../../data/unet_img/Data_TF_Scalerapid_Split/train_mask/\"\n",
    "train_img_gen = TFDataLoader(train_img_path, train_mask_path, num_class=9)          # calling TFDataLoader for training datasets  \n",
    "\n",
    "val_img_path = \"../../data/unet_img/Data_TF_Scalerapid_Split/val_image/\"\n",
    "val_mask_path = \"../../data/unet_img/Data_TF_Scalerapid_Split/val_mask/\"\n",
    "val_img_gen = TFDataLoader(val_img_path, val_mask_path, num_class=9)  \n",
    "\n",
    "test_img_path = \"../../data/unet_img/Data_TF_Scalerapid_Split/test_image/\"\n",
    "test_mask_path = \"../../data/unet_img/Data_TF_Scalerapid_Split/test_mask/\"\n",
    "test_img_gen = TFDataLoader(test_img_path, test_mask_path, num_class=9)\n",
    "\n",
    "\n",
    "# path for images from CVAT\n",
    "# path for both train, val datasets and test dataset\n",
    "\n",
    "# train_img_path = \"../../data/unet_img/Data_TF_397/train_image/\"                    \n",
    "# train_mask_path = \"../../data/unet_img/Data_TF_397/train_mask/\"\n",
    "# train_img_gen = TFDataLoader(train_img_path, train_mask_path, num_class=9)          # calling TFDataLoader for training datasets  \n",
    "\n",
    "# val_img_path = \"../../data/unet_img/Data_TF_397/val_image/\"\n",
    "# val_mask_path = \"../../data/unet_img/Data_TF_397/val_mask/\"\n",
    "# val_img_gen = TFDataLoader(val_img_path, val_mask_path, num_class=9)  \n",
    "\n",
    "# test_img_path = \"../../data/unet_img/Data_TF_397/test_image/\"\n",
    "# test_mask_path = \"../../data/unet_img/Data_TF_397/test_mask/\"\n",
    "# test_img_gen = TFDataLoader(test_img_path, test_mask_path, num_class=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_img_gen.__next__() # data iterator\n",
    "\n",
    "# checking/ verifying if the image and masks are coorelated\n",
    "for i in range(0,3):\n",
    "    image = x_train[i]\n",
    "    mask = np.argmax(y_train[i], axis=2)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(mask, cmap = 'gray' ) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = val_img_gen.__next__()\n",
    "for i in range(0,3):\n",
    "    image = x_val[i]\n",
    "    mask = np.argmax(y_val[i], axis=2)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# inputs for the UNET model\n",
    "IMG_HEIGHT = x_train.shape[1]\n",
    "IMG_WIDTH  = x_train.shape[2]\n",
    "IMG_CHANNELS = x_train.shape[3]\n",
    "print(IMG_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the loss function:\n",
    "dice_loss = sm.losses.DiceLoss()\n",
    "focal_loss = sm.losses.CategoricalFocalLoss(gamma=3)\n",
    "total_loss = dice_loss + (1*focal_loss)\n",
    "\n",
    "metrics = [sm.metrics.IOUScore(threshold = 0.5), sm.metrics.FScore(threshold = 0.5),'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n"
     ]
    }
   ],
   "source": [
    "# for scale.rapid\n",
    "train_img_path_len = \"../../data/unet_img/Data_TF_Scalerapid_Split/train_image/Original\"\n",
    "img_list_len = len(os.listdir(train_img_path_len))\n",
    "print(img_list_len)\n",
    "\n",
    "# for cvat\n",
    "# train_img_path_len = \"../../data/unet_img/Data_TF_397/train_image/JPEGImages\"\n",
    "# img_list_len = len(os.listdir(train_img_path_len))\n",
    "# print(img_list_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input for the model\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "# model intializing (using UNET from Segmentation model lib)    #n_classes\n",
    "model_3 = sm.Unet(BACKBONE_3, \n",
    "                encoder_weights = 'imagenet', \n",
    "                input_shape = input_shape, \n",
    "                classes = 9 ,\n",
    "                activation = 'softmax')\n",
    "learning_rate = 1e-5\n",
    "#model_3.trainable = False\n",
    "\n",
    "# loading weights:\n",
    "#model_3.load_weights('UNET_RESNET152_ScaleRapid_08Jun22.hdf5')\n",
    "model_3.compile(optimizer = Adam(learning_rate = learning_rate),\n",
    "                loss = total_loss, \n",
    "                metrics = metrics)\n",
    "#model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 40132), started 28 days, 0:26:54 ago. (Use '!kill 40132' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b23818dbd4587d3c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b23818dbd4587d3c\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize Tensorboard to monitor changes in Model Loss \n",
    "import datetime\n",
    "%load_ext tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "#Visualize on tensorboard (move this above)\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('PSP_RESNET152_ScaleRapid__.hdf5', monitor='loss',verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.9900 - iou_score: 0.0103 - f1-score: 0.0200 - accuracy: 0.2597\n",
      "Epoch 1: saving model to PSP_RESNET152_ScaleRapid__.hdf5\n",
      "51/51 [==============================] - 37s 501ms/step - loss: 0.9900 - iou_score: 0.0103 - f1-score: 0.0200 - accuracy: 0.2597 - val_loss: 1.0052 - val_iou_score: 1.3899e-08 - val_f1-score: 1.3899e-08 - val_accuracy: 0.0236\n",
      "Epoch 2/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.9700 - iou_score: 0.0282 - f1-score: 0.0515 - accuracy: 0.3599\n",
      "Epoch 2: saving model to PSP_RESNET152_ScaleRapid__.hdf5\n",
      "51/51 [==============================] - 24s 475ms/step - loss: 0.9700 - iou_score: 0.0282 - f1-score: 0.0515 - accuracy: 0.3599 - val_loss: 1.0019 - val_iou_score: 0.0022 - val_f1-score: 0.0022 - val_accuracy: 0.0979\n",
      "Epoch 3/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.9527 - iou_score: 0.0553 - f1-score: 0.0941 - accuracy: 0.4127\n",
      "Epoch 3: saving model to PSP_RESNET152_ScaleRapid__.hdf5\n",
      "51/51 [==============================] - 24s 475ms/step - loss: 0.9527 - iou_score: 0.0553 - f1-score: 0.0941 - accuracy: 0.4127 - val_loss: 0.9871 - val_iou_score: 0.0024 - val_f1-score: 0.0046 - val_accuracy: 0.2636\n",
      "Epoch 4/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.9376 - iou_score: 0.0862 - f1-score: 0.1398 - accuracy: 0.4526\n",
      "Epoch 4: saving model to PSP_RESNET152_ScaleRapid__.hdf5\n",
      "51/51 [==============================] - 24s 472ms/step - loss: 0.9376 - iou_score: 0.0862 - f1-score: 0.1398 - accuracy: 0.4526 - val_loss: 0.9708 - val_iou_score: 0.0151 - val_f1-score: 0.0266 - val_accuracy: 0.4065\n",
      "Epoch 5/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.9247 - iou_score: 0.1208 - f1-score: 0.1854 - accuracy: 0.4795\n",
      "Epoch 5: saving model to PSP_RESNET152_ScaleRapid__.hdf5\n",
      "51/51 [==============================] - 24s 474ms/step - loss: 0.9247 - iou_score: 0.1208 - f1-score: 0.1854 - accuracy: 0.4795 - val_loss: 0.9555 - val_iou_score: 0.0305 - val_f1-score: 0.0473 - val_accuracy: 0.5198\n",
      "Epoch 6/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.9141 - iou_score: 0.1461 - f1-score: 0.2146 - accuracy: 0.4967\n",
      "Epoch 6: saving model to PSP_RESNET152_ScaleRapid__.hdf5\n",
      "51/51 [==============================] - 24s 473ms/step - loss: 0.9141 - iou_score: 0.1461 - f1-score: 0.2146 - accuracy: 0.4967 - val_loss: 0.9407 - val_iou_score: 0.0464 - val_f1-score: 0.0654 - val_accuracy: 0.6253\n",
      "Epoch 7/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.9038 - iou_score: 0.1704 - f1-score: 0.2409 - accuracy: 0.5132\n",
      "Epoch 7: saving model to PSP_RESNET152_ScaleRapid__.hdf5\n",
      "51/51 [==============================] - 24s 473ms/step - loss: 0.9038 - iou_score: 0.1704 - f1-score: 0.2409 - accuracy: 0.5132 - val_loss: 0.9241 - val_iou_score: 0.0680 - val_f1-score: 0.0843 - val_accuracy: 0.7361\n",
      "Epoch 8/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.8936 - iou_score: 0.1859 - f1-score: 0.2593 - accuracy: 0.5315\n",
      "Epoch 8: saving model to PSP_RESNET152_ScaleRapid__.hdf5\n",
      "51/51 [==============================] - 24s 473ms/step - loss: 0.8936 - iou_score: 0.1859 - f1-score: 0.2593 - accuracy: 0.5315 - val_loss: 0.9179 - val_iou_score: 0.0798 - val_f1-score: 0.0936 - val_accuracy: 0.7533\n",
      "Epoch 9/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.8859 - iou_score: 0.2027 - f1-score: 0.2778 - accuracy: 0.5437\n",
      "Epoch 9: saving model to PSP_RESNET152_ScaleRapid__.hdf5\n",
      "51/51 [==============================] - 24s 469ms/step - loss: 0.8859 - iou_score: 0.2027 - f1-score: 0.2778 - accuracy: 0.5437 - val_loss: 0.9141 - val_iou_score: 0.0836 - val_f1-score: 0.0954 - val_accuracy: 0.7868\n",
      "Epoch 10/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.8761 - iou_score: 0.2176 - f1-score: 0.2949 - accuracy: 0.5609\n",
      "Epoch 10: saving model to PSP_RESNET152_ScaleRapid__.hdf5\n",
      "51/51 [==============================] - 24s 474ms/step - loss: 0.8761 - iou_score: 0.2176 - f1-score: 0.2949 - accuracy: 0.5609 - val_loss: 0.9131 - val_iou_score: 0.0840 - val_f1-score: 0.0956 - val_accuracy: 0.7823\n"
     ]
    }
   ],
   "source": [
    "# model fitting\n",
    "steps_per_epoch = img_list_len //batch_size\n",
    "history_3 = model_3.fit(train_img_gen,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs = 10,\n",
    "                    validation_data=val_img_gen, \n",
    "                    validation_steps=steps_per_epoch,\n",
    "                    verbose = 1, callbacks = [model_checkpoint, tensorboard_callback] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation\n",
    "print(\"____________________________________________________________\")\n",
    "print(\"____________________________________________________________\")\n",
    "print(\"____________________________________________________________\")\n",
    "train_IoU = model_3.evaluate(train_img_gen,\n",
    "                                batch_size = batch_size,\n",
    "                                steps = steps_per_epoch)\n",
    "print(\"Train IoU is = \", (train_IoU[1] * 100.0), \"%\")\n",
    "\n",
    "val_IoU = model_3.evaluate(val_img_gen,\n",
    "                                batch_size = batch_size,\n",
    "                                steps = steps_per_epoch)\n",
    "print(\"Val IoU is = \", (val_IoU[1] * 100.0), \"%\")\n",
    "\n",
    "print(\"____________________________________________________________\")\n",
    "print(\"____________________________________________________________\")\n",
    "print(\"____________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.save('PSP_RESNET152_ScaleRapid.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training and validation loss at each epoch\n",
    "loss = history_3.history['loss']\n",
    "val_loss = history_3.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation IoU for each epoch\n",
    "iou_train = history_3.history['iou_score']\n",
    "iou_val = history_3.history['val_iou_score']\n",
    "\n",
    "plt.plot(epochs, iou_train, 'y', label='Training IoU')\n",
    "plt.plot(epochs, iou_val, 'r', label='Validation IoU')\n",
    "plt.title('Training and validation IoU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation F_score for each epoch\n",
    "iou_train = history_3.history['f1-score']\n",
    "iou_val = history_3.history['val_f1-score']\n",
    "\n",
    "plt.plot(epochs, iou_train, 'y', label='Training F1')\n",
    "plt.plot(epochs, iou_val, 'r', label='Validation F1')\n",
    "plt.title('Training and validation F1')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the model\n",
    "from keras.models import load_model\n",
    "model_effnet = load_model('PSP_RESNET152_ScaleRapid.hdf5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_batch, test_mask_batch = test_img_gen.__next__()\n",
    "\n",
    "#Convert categorical to integer for visualization and IoU calculation\n",
    "test_mask_batch_argmax = np.argmax(test_mask_batch, axis=3) \n",
    "test_pred_batch = model_effnet.predict(test_image_batch)\n",
    "test_pred_batch_argmax = np.argmax(test_pred_batch, axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating MeanIoU (intersection over Union)\n",
    "n_classes = 9\n",
    "IOU_keras_mod3 = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras_mod3.update_state(test_pred_batch_argmax, test_mask_batch_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras_mod3.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating IoU for individual labels\n",
    "import pandas as pd\n",
    "\n",
    "weight_values_3 = np.array(IOU_keras_mod3.get_weights()).reshape(n_classes, n_classes)\n",
    "#classes = ['Background','Wings','Proboscis','Head','Antennae','Palps','Abdomen','Legs','Thorax'] # for cvat\n",
    "classes = ['Background','Palps','Abdomen','Thorax','Antennae','Wing','Head','Proboscis','Legs'] # for scale.rapid\n",
    "#classes = ['Palps','Abdomen','Thorax','Antennae','Wing','Head','Proboscis','Legs'] # for scale.rapid with no background\n",
    "weight_val_df = pd.DataFrame(weight_values_3, index = classes, columns= classes)\n",
    "\n",
    "def IoU_classes(n_classes,weight_values, classes):\n",
    "    \"\"\" Calculate IoU for each class or label\"\"\"\n",
    "    # initializing a dict to store all the IoU values for each label or class\n",
    "    IoU_individual_classes = {}\n",
    "    for i , j, label in zip(np.arange(n_classes), np.arange(n_classes), classes):\n",
    "        #IoU_individual_classes[\"classes_{0}\".format(i)] = weight_values[i,j]/(np.sum(weight_values[:,i]) + np.sum(weight_values[j]) - weight_values[i,j])\n",
    "        IoU_individual_classes[label] = weight_values[i,j]/(np.sum(weight_values[:,i]) + np.sum(weight_values[j]) - weight_values[i,j])\n",
    "    IoU_all_classes = pd.DataFrame([IoU_individual_classes])\n",
    "    return IoU_all_classes\n",
    "\n",
    "IoU_classes_3 = IoU_classes(n_classes,weight_values_3,classes)\n",
    "filename_csv = 'PSP_RESNET152_ScaleRapid.csv'\n",
    "IoU_classes_3.to_csv(filename_csv)\n",
    "print(IoU_classes_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function for multilabel confusion matrix\n",
    "def MultiClassConfussionMatrix(corr_data, classes):\n",
    "    ground_truth = corr_data.sum(axis = 1)                  # summing all the ground truth (sum all the rows)                             \n",
    "    corr_data_norm = corr_data/ground_truth.reshape(-1,1)   # normalizing \n",
    "    corr_data_norm_df = pd.DataFrame(corr_data_norm,        # building a dataframe\n",
    "                                    index = classes, \n",
    "                                    columns= classes)\n",
    "    plt.figure(figsize=(10, 10))                            # setting the size of the figure\n",
    "    corr_mat = sns.heatmap(corr_data_norm_df,               # plotting \n",
    "                        annot=True, \n",
    "                        fmt=\".3f\",\n",
    "                        cmap='BuPu')\n",
    "    figname = 'corr_mat_PSP_RESNET152_ScaleRapid.png'\n",
    "    plt.savefig(figname)\n",
    "    return corr_mat\n",
    "    \n",
    "\n",
    "MultiClassConfussionMatrix(weight_values_3,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View a few images, masks and corresponding predictions. \n",
    "img_list = random.sample(range(0, test_image_batch.shape[0]-1), batch_size-1)\n",
    "#img_num = 2\n",
    "for img_num in img_list:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(231)\n",
    "    plt.title('Testing Image')\n",
    "    plt.imshow(test_image_batch[img_num])\n",
    "    plt.subplot(232)\n",
    "    plt.title('Testing Label')\n",
    "    plt.imshow(test_mask_batch_argmax[img_num])\n",
    "    plt.subplot(233)\n",
    "    plt.title('Prediction on test image')\n",
    "    plt.imshow(test_pred_batch_argmax[img_num])\n",
    "\n",
    "    figname = f'{img_num}.png'\n",
    "    plt.savefig(figname)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b90cab7ea642421f44636989edaf96d86cb1abe354b45ce6eed3b362842c2584"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('deepL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
